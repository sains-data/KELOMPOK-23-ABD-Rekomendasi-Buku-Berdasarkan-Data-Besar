# Anda bisa hapus baris 'version' ini jika Docker Compose Anda versi baru
version: '3.8'

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870" # NameNode Web UI
      - "9000:9000" # FS Default Name (used by Spark, Hive, etc.)
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode

  # --- Spark ---
  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080" # Spark Master Web UI
      - "7077:7077" # Spark Master Port
    environment:
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./script:/opt/spark_apps # Path lokal: ./script, di container /opt/spark_apps
      - ./data_source:/opt/data_source # Untuk data CSV awal

  spark-worker:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081" # Spark Worker Web UI
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    volumes:
      - ./script:/opt/spark_apps # Path lokal: ./script, di container /opt/spark_apps
      - ./data_source:/opt/data_source

  # --- Hive ---
  hive-metastore-db:
    image: postgres:12
    container_name: hive-metastore-db
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword # Ganti dengan password yang lebih kuat di produksi
      POSTGRES_DB: metastore
    volumes:
      - hive_metastore_db_data:/var/lib/postgresql/data

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9000 hive-metastore-db:5432"
      DB_DRIVER: "org.postgresql.Driver"
    ports:
      - "9083:9083"
    volumes:
      - ./config/hive/hive-site.xml:/opt/hive/conf/hive-site.xml
    depends_on:
      - namenode
      - datanode
      - hive-metastore-db

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop.env
    environment:
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
      - "10002:10002"
    volumes:
      - ./config/hive/hive-site.xml:/opt/hive/conf/hive-site.xml
    depends_on:
      - hive-metastore

  # --- Airflow ---
  postgres-airflow:
    image: postgres:13
    container_name: postgres-airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow # Ganti dengan password yang lebih kuat di produksi
      - POSTGRES_DB=airflow
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"

  redis-airflow:
    image: redis:latest
    container_name: redis-airflow
    ports:
      - "6379:6379"

  airflow-init:
    image: puckel/docker-airflow:1.10.9 # Pertimbangkan untuk upgrade ke versi Airflow lebih baru
    container_name: airflow-init
    depends_on:
      - postgres-airflow
      - redis-airflow
    env_file:
      - ./airflow.env
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow version
        if [ ! -f "/usr/local/airflow/airflowdb_initialized" ]; then
          echo "Initializing Airflow DB..."
          airflow db init && \
          airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin && \
          airflow connections add 'spark_default' --conn-type 'spark' --conn-host 'spark://spark-master:7077' && \
          touch /usr/local/airflow/airflowdb_initialized
        else
          echo "Airflow DB already initialized."
        fi
    volumes: # Volume untuk menandai inisialisasi
      - airflow_init_marker:/usr/local/airflow 

  airflow-webserver:
    image: puckel/docker-airflow:1.10.9
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres-airflow
      - redis-airflow
      - airflow-init
    ports:
      - "8088:8080"
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./script:/opt/spark_apps
      - ./data_source:/opt/data_source
    env_file:
      - ./airflow.env
    command: webserver

  airflow-scheduler:
    image: puckel/docker-airflow:1.10.9
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - postgres-airflow
      - redis-airflow
      - airflow-init
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./script:/opt/spark_apps
      - ./data_source:/opt/data_source
      - airflow_init_marker:/usr/local/airflow # Agar scheduler menunggu init selesai
    env_file:
      - ./airflow.env
    command: scheduler

  airflow-worker:
    image: puckel/docker-airflow:1.10.9
    container_name: airflow-worker
    restart: always
    depends_on:
      - postgres-airflow
      - redis-airflow
      - airflow-scheduler # Worker tergantung pada scheduler
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./script:/opt/spark_apps
      - ./data_source:/opt/data_source
    env_file:
      - ./airflow.env
    command: worker

  # --- Superset ---
  superset-db:
    image: postgres:12
    container_name: superset-db
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: supersetpassword # Ganti dengan password yang lebih kuat di produksi
      POSTGRES_DB: superset
    volumes:
      - superset_db_data:/var/lib/postgresql/data
    ports:
      - "5434:5432"

  superset:
    image: apache/superset:latest # Pertimbangkan versi spesifik untuk stabilitas
    container_name: superset
    depends_on:
      - superset-db
    ports:
      - "8089:8088"
    environment:
      ADMIN_USERNAME: admin
      ADMIN_EMAIL: admin@superset.com
      ADMIN_PASSWORD: admin # Ganti dengan password yang lebih kuat di produksi
      SUPERSET_SECRET_KEY: "ganti_dengan_kunci_acak_kuat_anda_disini_12345" # <--- GANTI INI DENGAN KUNCI ACAK KUAT ANDA!
      SQLALCHEMY_DATABASE_URI: "postgresql+psycopg2://superset:supersetpassword@superset-db:5432/superset"
      FLASK_APP: "superset" # Diperlukan untuk inisialisasi Superset di beberapa versi
    volumes:
      - superset_data:/app/superset_home
      - ./config/superset/superset_config.py:/app/pythonpath/superset_config.py
    command: >
      bash -c "superset db upgrade && \
               superset init && \
               superset load_examples --force && \
               superset run -p 8088 --with-threads --reload --debugger"


volumes:
  hadoop_namenode:
  hadoop_datanode:
  hive_metastore_db_data:
  airflow_db_data:
  airflow_init_marker: # Volume untuk menandai inisialisasi Airflow
  superset_db_data:
  superset_data: